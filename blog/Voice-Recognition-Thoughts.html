<html lang="en" resource-type="blogpost"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@briankardell">
    <meta name="twitter:creator" content="@briankardell">
    <meta name="twitter:title" content="Thoughts on Voice Recognition: Web Speech APIs Part IV">
    <meta name="twitter:description" content="This is part of a series about making the browser speak and listen to speech. In my last post You Don't Say: Web Speech APIs Part III, I talked about the recognition interfaces: li">
    
    <link rel="alternate" type="application/rss+xml" href="https://bkardell.com/blog/feed.rss" title="bkardell.com/blog rss feed">
    <title>Thoughts on Voice Recognition: Web Speech APIs Part IV</title> 
    <style>.captioned-image {
    background-color: #eaeaea;
    display: block;
    overflow: hidden;
    padding: 1rem;
    text-align: center;
    font-style: italic;
}

.captioned-image.p-attached {
    width: 40%;
    display: inline-block;
    margin: 0 1rem 2rem 1rem;
}

section > .captioned-image.p-attached {
    margin-top: 1.5rem; /* correct for heading size*/
}

 pre { overflow-x: auto }

.captioned-image.p-attached.p-attached-left {
    float: left;
}

.captioned-image.p-attached.p-attached-right {
    float: right;
}

.captioned-image.p-attached + * + * {
    clear: both;
    margin-top: 2rem;
}


.captioned-image img, .captioned-image video {
    display: block;
    max-width: 100%;
    margin: 0 auto 1em auto;
}
.source {
    font-style: italic;
}
body {
    display: flex;
    font: 1em "Helvetica Neue", Helvetica, Arial, sans-serif;
    font-weight: 400;
    line-height: 1.625;
}
h1 { margin: 0.5rem; }
code-format {
    border-left: 0.5rem solid rgba(123,115,209,0.35);
    display: block;
    padding: 0.5rem;
    margin-bottom: 1rem;
}

contextual-heading {
    display: block;
    margin-top: 2rem;
}

[aria-level="1"] {
    color: #43686b;
    font-size: 1.45rem;
}
[aria-level="2"] {
    color: #856363;
    font-size: 1.35rem;
}
[aria-level="3"] {
    color: #856363;
    font-size: 1.25rem;
}
[aria-level="4"] {
    color: #856363;
    font-size: 1.2rem;
}
[aria-level="5"], [aria-level="6"] {
    color: #667496;
    font-size: 1.1rem;
    font-weight: bolder;
}
.posted-on {
    font-style: italic;
    font-size: 0.8rem;
    text-align: right;
    margin-bottom: 1rem;
}

ul, li {
    margin: 0.5rem;
    text-align: left;
}
header {
    padding-top: 1rem;
    padding-right: 2rem;
    background-color: #fbf5e9;
}
.tagline {
    font-style: italic;
    text-align: center;
}

[tag-esc] {
    color: maroon;
    font-family: Courier, "Lucida Console", monospace;
}

[tag-esc]::before {
    content: '<';
}

[tag-esc]::after {
    content: '>';
}

main {
    min-width: 50%;
    width: 80%;
    padding: 1rem;
}
header .title {
    background-color: rgba(123,115,209,0.35);
    padding: 0.3rem;
    padding-left: 0.25rem;
    font-weight: bolder;
    font-size: 0.9rem;
    border-left: 1rem solid #999298;
}

header .profile {
    width: 50%;
    margin: 1rem 25%;
}

header .title + nav {
    font-size: 0.9rem;
}

header .blurb {
    font-size: 0.75rem;
    text-align: center;
}
header .name {
    text-align: center;
    font-size: xx-large;
}
.segue {
    font-style: italic;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0,0,0,0);
  border: 0;
}
[for="aboutMeToggle"] {
    display: none;
}
/* this needs rethinking, it wants a selector like 'not flow content' */
article>*:not(contextual-heading,a), section>*:not(contextual-heading,a) {
    margin-left: 0.45rem;
}

blockquote {
  font-family: serif;
  background: #f9f9f9;
  border-left: 10px solid #ccc;
  margin: 1.5em 10px;
  padding: 0.5em 10px;
}
blockquote:before {
  color: #ccc;
  content: open-quote;
  font-size: 2em;
  line-height: 0.1em;
  margin-right: 0.25em;
  vertical-align: -0.4em;
}

blockquote:after {
  color: #ccc;
  content: close-quote;
  font-size: 2em;
  line-height: 0.1em;
  margin-left: 0.25em;
  vertical-align: -0.4em;
}


@media (max-width: 800px){
    body {
        flex-direction: column;
    }
    header {
        border-bottom: 1px solid #afafff;
    }

    .captioned-image.p-attached {
        width: 100%;
        margin: 1rem 0;
    }

    .captioned-image.p-attached.p-attached-left,
    .captioned-image.p-attached.p-attached-right {
        float: none;
    }

    [for="aboutMeToggle"] {
        background-color: #675e5e;
        color: white;
        padding: 0.25rem;
        padding-left: 0.5rem;
        display: block;
        border-left: 1rem solid black;
    }
    #aboutMeToggle:focus + [for="aboutMeToggle"]{
        outline: 4px solid #c6ddf2;
    }

    #aboutMeToggle:checked ~ header {
        display: none;
    }

    header li { display: inline; margin-left: -0.5em; }
    header .blurb li::after { content: '; '; }
    header  nav li {
        display: inline-block;
        margin-left: 0.35rem;
        margin-right: 0.35rem;
    }
    header .title {
        width: 100%;
    }
    main li .source { display: block; }

}</style>
    <style>
      .captioned-image { font-size: 0.9rem; }
      .thanksTo { font-style: italic; font-size: 0.8rem; }
    </style>
    <script>
      var activateOptional = function (media) {
        var source = media.getAttribute('data-src'),
            temp,
            container = media.parentElement;

          if (media.nextSibling && media.nextSibling.matches) {
            if (media.nextSibling.matches('.clickToSee')) {
              media.parentElement.removeChild(media.nextSibling);
            }
          }

          if (media.tagName === 'VIDEO') {
            temp = document.createElement('video')
            temp.setAttribute('autoplay','')
            temp.setAttribute('controls','')
            temp.setAttribute('loop','')
            temp.innerHTML = '<source src="' + source + '.webm" type="video/webm">'
                    + '<source src="' + source + '.mp4" type="video/mp4">';
            media.parentElement.replaceChild(temp, media);
            setTimeout(function () {
              temp.play()
            }, 0);
          } else {
            media.src = media.getAttribute('data-src');
          }

          container.classList.add('active');
      }
    </script>
  </head>
  <body>
    <input id="aboutMeToggle" class="sr-only" type="checkbox" checked="">
<label for="aboutMeToggle">Toggle author information</label>
<header>
    <div class="name">
        Brian Kardell
    </div>
    <img class="profile" src="/profile.jpg" alt="">
    <div class="tagline"><a href="/">Betterifying the Web</a></div>
    <div class="blurb">
        <ul>
            <li>Developer Advocate at Igalia</li>
            <li>Original Co-author/Co-signer of The Extensible Web Manifesto</li>
            <li>Co-Founder/Chair, W3C Extensible Web CG</li>
            <li>Member, W3C (The JS Foundation)</li>
            <li>Co-author of HitchJS</li>
            <li>Blogger</li>
            <li>Art, Science &amp; History Lover</li>
            <li>Standards Geek</li>
        </ul>
    </div>
    <div class="title">Follow Me On...</div>
    <nav>
        <ul>
            <li><a rel="me" href="https://briankardell.wordpress.com">Wordpress</a></li>
            <li><a rel="me" href="https://medium.com/@briankardell">Medium</a></li>
            <li><a rel="me" href="https://twitter.com/briankardell">Twitter</a></li>
            <li><a rel="me" href="https://github.com/bkardell/">Github</a></li>
            <li><a rel="me" href="https://codepen.io/bkardell">Codepen</a></li>
            <li><a rel="me" href="https://www.linkedin.com/in/brian-kardell-08a4264">LinkedIn</a></li>
            <li><a rel="me" href="https://www.instagram.com/kardellbrian">Instagram (for
art)</a></li>
            <li><a rel="me" href="http://fineartamerica.com/profiles/brian-kardell?">Fine Art
America (art for sale)</a></li>
        </ul>
    </nav>
</header>
    <main><div class="posted-on">Posted on 09/17/2017</div><article posted-on="09/17/2017" class="sectioning">
  <contextual-heading role="heading" aria-level="1">Thoughts on Voice Recognition: Web Speech APIs Part IV</contextual-heading>
  <p class="segue">This is part of a series about making the browser speak and listen to speech. In my last post <a href="https://bkardell.com/blog/Listen-Up.html">You Don't Say: Web Speech APIs Part III</a>, I talked about the recognition interfaces: <em>listening</em> or <em>speech-to-text</em>.</p>

  <script src="../prism.js"></script>
  <script src="../test/listening/basic-voice-listener.js"></script>
  <script src="../test/listening/pausable-voice-listener.js"></script>
  <script src="../test/listening/VoiceInputElement.js"></script>
  <link rel="stylesheet" href="../prism.css">
  <style data-id="AIzaSyBcqG9hwjurgnXOQy8yGYahRICNl2TUbuI">

    .note {
      background-color: rgba(255, 255, 0, 0.24);
      padding: 1rem;
      font-style: italic;
    }
    .note::before {
      content: 'Note';
      font-size: 0.8rem;
      background-color: black;
      color: white;
      padding: 0.25rem;
      margin-right: 0.5rem;
    }
    .output {
      margin-left: 1rem;
      font-family: "Courier New", Courier, monospace;
    }
    pre code.output {
      font-size: 0.7rem;
      border-left: 1px dotted gray;
      padding-left: 1rem;
      display: block;
    }

    .spec-quote {
      display: block;
      margin: 1rem;
    }
  </style>
  <script>
    document.body.addEventListener("click", function(evt) {
      if (evt.target.classList.contains("run")) {
        let clone = document.importNode(
          evt.target.previousElementSibling.content,
          true
        );
        document.body.appendChild(clone);
      } else if (evt.target.classList.contains("cancel")) {
        speechSynthesis.cancel();
        speechSynthesis.speak(
          new SpeechSynthesisUtterance("normalcy should be restored")
        );
      }
    });
  </script>

  <p>To sum up the state of things with recognition:</p>

  <ol>
    <li>It's <em>much</em> less widely implemented and deployed than Text-To-Speech (TTS).</li>
    <li>Like TTS, it is not an 'official' standard. Browsers are working on it, but not much work seems to be happening 'together' on this anymore.</li>
    <li>It requires special privileges</li>
    <li>Its API is kind of confusing</li>
  </ol>

  <section class="sectioning">
    <contextual-heading role="heading" aria-level="2">What's wrong with the current APIs</contextual-heading>
    <p class="note">These are just my personal opinions after using it a lot for the past couple of months.
    Sometimes eating the dogfood gives you new perspectives and nothing in this should be taken as a
    criticism of the brave souls that worked hard to bring us what we have.  It's easy to forget that we're standing on their shoulders. Hats off to them.</p>

    <p>Let's assume that the API worked consistently, as advertised by the existing draft, across browsers and on all devices. The trouble with is that I see is mostly just that it doesn't "fit" in the Web platform today.  It's not really a high level API, nor is it quite the low level we probably need.
    </p>

    <section class="sectioning">
      <contextual-heading role="heading" aria-level="3">Imagine...</contextual-heading>
      <p>When I hear "voice recognition" or "speech to text", this implies to me that I can take a sound,
          analyze it and turn it into a transcipt of text.  When I watch a demo and I see someone speak into the mic and words come out on the screen, I have the same impression.  I get excited and begin thinking of the possibilities. Yes, I want that.  Think of all the things I could do with the ability to turn sounds of spoken word into text - I begin to imagine...
          </p>

          <ul>
            <li>I turn the directory of podcasts I've downloaded into text. I could do lots of interesting things with that, skim them for example to see if I want to listen.  Better still, I can index them and hyperlink them.. I can make a word cloud... I can search for things that were discussed without trying to remember what was said about Web Components in .. hmm some a podcast? or was it a talk? It was Jeremy Keith... I think? Last year maybe? In fact, I don't even have to have watched it to know that.</li>

            <li>Speaking of podcasts, I've been thinking of doing one.  Imagine I'm in a studio with 4 other people all with their own mics.  It'd be super neat to identify them by name and capture their transcribed text in a simple transcript.  Hey, that's kind of like meetings.  I attend a lot of meetings that aren't minuted.  We could fix that.</li>

            <li>I have a thing that lets me upload a video to a website I maintain... I know that for accessibility sake I need captions of this video and that has to be in Timed Text Markup Language (TTML).  Wow, that's a pain in the ass to write from scratch.  I could write a thing to take the audio track, turns chunks of it into text, inserts that into TTML and uploads me a draft in my workflow that I can then simply proof, correct,  approve and publish.  We can't do that with this API either.</li>

            <li>I also had an idea for simplistic, hands-free conversational assistant for while I am driving - kind of my own simplified Jarvis.</li>
          </ul>

          <p>But there's the rub, none of these are actually easily accomplishable with the current API for various reasons, and mostly simply because it is kind of hard-wired toward that sound coming from the mic without really exposing that.</p>

          <p>This is a shame because since <a href="https://extensiblewebmanifesto.org">The Extensible Web Manifesto</a> (EWM), we've worked very hard to help "explain the magic" already burried in features of the platform and focus our initial efforts on the powers that are fundamentally new. As far as I can tell, really the only thing in these APIs that is fundamentally new is the idea of an interface for transcibing audio of speech into text.  Things like promises, web audio, media devices, fetch and streams are all things we've worked on and that can potentially be used to help explain just about everything else in this API.  As the speech draft predates all this, it's no suprise that none of that is taken into account, but it's not too late.</p>

          <p class="note">In fact, the authors of the spec did recognize some of this and there are notes in the spec and errata that link to, for example <a href="https://lists.w3.org/Archives/Public/public-speech-api/2012Sep/0072.html">this observation</a> with the thought that perhaps it was too late for this draft, as it is actually a 'final report' and not an official spec.  I hope we have room to revisit this.</p>

          <p>Let's use this as an exersize to start low and build up the concepts we want, as each level affords new opportunities for people to dream up things we weren't even considering.
          I've <a href="http://bkardell.com/blog/Prognostication-And-The-Failure-Of-The-Web.html">written about this before</a>, but it's astonishing how many creations that are fundamental things today turned out to <em>not</em> be what their inventors even thought they would be. Houses were originally wired for artifical lighting  - there were no plugs, because there was nothing to plug in.  While we were busy standardizing bulbs, an astonishing flourishing of inventions and whole new industries were created by the simple fact that we gave people electricity.
          </p>
      </section>


      <section class="sectioning">
        <contextual-heading role="heading" aria-level="3">A 'low-level' transcription interface</contextual-heading>
        <p>Let's imagine that we started simply with the idea of an API that gave us just the fundamentally new part: Something that takes some audio and gives you back the transcribed text.  Accessing your filesystem or your mic are things that require permission, but what we're talking about here doesn't itself do that, so that's not even something we need to worry about.  Actually, asking for analysis of something could be modeled after the HTTP request/response model, and that seems kind of fitting because there's potentially some variance in the response.  For example, different implementations can definitely have different interpretations of the same sounds.  So, let's start with the common model of being simple async and transactional, pass/fail things that we deal with via promises and resolve some kind of response...something like:</p>

        <pre><code class="language-javascript">// let's save bikeshedding a name for another day...
let transcriber = new FictionalTranscriber()

transcriber.transcribe(someAudioStream).then((response) =&gt; {
  // we'll talk more about this
})</code></pre>

        <p>That's pretty straightfoward I think. A transcriber has a <code>response</code> and an error is handled with the promise's <code>.catch</code> method.</p>

        <section class="sectioning">
          <contextual-heading role="heading" aria-level="4">Results</contextual-heading>
          <p>While the current interface you get these wonky array-likes which default to having exactly one
          item in them, but let's kind of work backwards and see if we can avoid this.. Imagine that the response itself simply contains the properties <code>.transcript</code> and <code>.confidence</code> (from the existing APIs).  Then the default 'just give me your best guess' case is as simple as:</p>

          <pre><code class="language-javascript">let service = new FictionalTranscriber()

transcriber.transcribe(someAudioStream).then((response) =&gt; {
  console.log(`
    I am ${response.confidence}% confident
    that I heard "${response.transcript}".
  `)
})</code></pre>

          <p>The existing APIs also contain a separate event called <code>onnomatch</code> that is
          different from <code>onerror</code> in that it's more about it receiving
          something and processing it but not being able to confidently enough
          deterimine what was said. It seems that what we mean to say is "we recognized nothing" or
          "we heard something but what it was is undefined".  In both cases these have
          programming concepts for these: The null string ("") or, in JavaScript we also have <code>undefined</code>.  In JavaScript are falsey - so I think we can get rid of that extra event
          entirely with something as simple as saying that that is a possiblility...</p>


        <pre><code class="language-javascript">let transcriber = new FictionalTranscriber()

transcriber.transcribe(someAudioStream).then((response) =&gt; {
  // if we didn't match anything sufficiently
  if (!response.transcript) {
     console.log(`Sorry, I didn't catch that`)
  } else {
    console.log(`
      I am ${response.confidence}% confident
      that I heard "${response.transcript}".
    `)
  }

})</code></pre>


          <section class="sectioning">
            <contextual-heading role="heading" aria-level="5">Alternatives</contextual-heading>
            <p>"Ah," you're thinking, "but those arrays are meaningful... One of them contains N possible 'alternatives'".   True, but alternative to what even?  This seems like a strange use of the word anyway.  Conversely, our model could also allow our response to contain an <code>.alternatives</code> array beyond our best guess transcript.  By default, it would be empty because there are no alternatives. This seems to actually match the meaning of the word "alternative" better anyway. Asking, for them seems like it could simply be an optional configuration...</p>

          <pre><code class="language-javascript">let transcriber = new FictionalTranscriber({maxAlternatives: 10})

transcriber.transcribe(someAudioStream).then((response) =&gt; {
  console.log(`
    I am ${response.confidence}% confident
    that I heard "${response.transcript}".
  `)

  if (response.alternatives) {
    console.log(`But here are ${evt.alternatives} alternatives... `)
    response.alternatives.forEach((alternative) =&gt; {
      console.log(`
        I am ${alternative.confidence}% sure it could
        have been "${alternative.transcript}"
      `)
    })
  }

})</code></pre>
          </section>

          <section class="sectioning">
            <contextual-heading role="heading" aria-level="5">Implementation Flexibility and Black Boxing</contextual-heading>
            <p>The existing API designs seem to want to allow you to specify a serviceURI and that sort of thing,
            but it seems unimplemented and there seems to be some kind of hand-waving on just how that would work, because there are competing ideas.</p>

            <p>One of the extra handy things about this kind of design, I think, is that it also neatly sidesteps a number of problems that are impossibly difficult to resolve at this stage by boxing those problems at an appropriate place where there is easier to reach value and yet room for experimentation.  Let me explain...</p>

            <p>What we are providing what is, effectively, a service interface.  We're not saying how the actual
            transciption should be done.  There are an astounding amount of challenges there.  There are lots of different existing remote services with lots of different qualities, lots of different APIs and lot of different sorts of inputs. A want to figure all of this out will stall a process for a very long time and be, ultimately, as much about politics and sway as actual merit.</p>

            <p>On the other hand, if we simply say "Here's what it has to look like on the client, and the client should include a default implementation" we have to agree to very little.  While this seems like it isn't giving you 'much', the adoption of this simple pattern would provide the standard we most desparately need:  All we have to agree on at this stage is what the client interface looks like.</p>

            <p>There are some other practical upshots to this too.  The first is that we can also make that extensible by defining basic API lifecycle methods and allow competition and variance to play itself out very easily and employ lines of thinking we can't easily consider in a standards body.  Imagine, for example, that Google's cloud platform could release a subclass:</p>

            <pre><code class="language-javascript">export default class FictionalGoogleCloudTranscriber
  extends FictionalTranscriber {
    // override
    request: (audio) =&gt; {
      // return some promise that meets the expected API
      return fetch(....).then(...)
    }
}</code></pre>

          <p>The only thing that changes is which one you instantiate - they all work, at the API level, the same way.  This means that these adapters are then as easily distributable as jQuery plugins and Amazon can compete with Google and Microsoft and Mozilla and... well... anyone while we figure out </p>

          <p>This also fits very nicely with allowing us to experiment with the remaining (seemingly unimplemented) ideas in the APIs (like <em>grammars</em>) and provides room for us to work things out.  Eventually, perhaps we can derive a higher level protocol allowing us to express things as simply as:</p>

          <pre><code class="language-javascript">let transcriber = new FictionalTranscriber({
  serviceURI: 'blah',
  grammars: whateverThisIs
})

transcriber.transcribe(someAudioStream).then((response) =&gt; {
  console.log(`
    I am ${response.confidence}% confident
    that I heard "${response.transcript}".
  `)
})</code></pre>

            <p>Importantly though, this means we don't have to wait for that day and we can be part of the solution in figuring out what it should be.</p>

            <p>Interestingly, there was <a href="https://docs.google.com/document/d/1QRajFwBdV82aTnPCQNKOfL2ZGkyfG9X9ZfCYgK2xstk/edit" target="_blank">another proposal</a>, also from someone at Google
        more recently that is considerably closer to this idea.  However, that idea
        seems to have not gone far.</p>
          </section>
        </section>
        </section>
      </section>


        <section class="sectioning">
          <contextual-heading role="heading" aria-level="2">Low Level <em>First</em>, Not Exclusively</contextual-heading>
          <p>The idea isn't ever to stop at the low level APIs.  It's not even that we
          absolutely positively have to have the low level APIs entirely in place.
          As I explain in <a href="https://bkardell.com/blog/Tao-Of-The-Extensible-Web.html">What Would Bruce Lee Do? The Tao of the Extensible Web</a>, in some cases we can come back and expose details of a carefully designed black box after the fact.  At a minimum, describing the API in common platform terms and the thought exercise of what a suitable low level API would look like allows us to design the boxes and
          layers carefully.</p>

          <p>For example, given such an API as described above, it <em>would</em> make sense to add a
          higher level API which made use of it and exposed it... A pattern which explained
          the common lifecycle of grabbing a device, getting permission, listening to a channel, chunking that input, sending it to the transcriber and ultimately pumping out transcripts.  Perhaps something like..
          </p>

          <pre><code class="language-javascript">// Here we create an object not entirely unlike what
// the API already had - it defaults to being initialized
// with the audio context/primary mic, sends a chunk
// of sound to our transcriber and relays the promise
// results or errors to events
let speechRecognizer = new FictionalSpeechRecognizer()


// event based methods relay promise results
speechRecognizer.onresult = (evt) =&gt; { /* evt.response.transcript */ }
speechRecognizer.onerror = (evt) =&gt; { /* evt.error */ }

// the audioContext is exposed: You can control it
speechRecognizer.audioContext.start()
speechRecognizer.audioContext.stop()</code></pre>

        <p>The general idea being that we can answer the questions "what's under there?"
        by saying "it has an audio context and a transcriber" and we can expose those
        for extensibility... For example, we could allow you to swap out the transcriber
        implementation without forcing you to reimagine any other part in the system..</p>

         <pre><code class="language-javascript">let speechRecognizer = new FictionalSpeechRecognizer({
  transcriber: new FictionalGoogleCloudTranscriber()
})</code></pre>

        <p>Or if you can get an audio context another way (say, one of N line inputs, or from a file),
        you can swap that out easily without forcing you to reimagine anything else either..</p>

        <pre><code class="language-javascript">let speechRecognizer = new FictionalSpeechRecognizer({
  audioContext: someAudioContext
})</code></pre>

        <p>Given such an explanation, we could easily experiment with further layers, floating ideas as
        custom elements.  For example: An <code tag-esc="">input</code> decorator that could progressively
        enhance an textual form controls to allow you to populate them with speech.  All you'd need
        to say is 'they have one of those speech recognizer things and provide a toggle that pipes
        voice to it and the output back into the field'.. Something like this.. </p>

        <pre><code class="language-html">
&lt;x-voice-listener&gt;
  &lt;input name="input"&gt;
&lt;/x-voice-listener&gt;
        </code></pre>
        <x-voice-listener>
            <input placeholder="How can I help?">
        </x-voice-listener>

        <p>That's kind of interesting because there were <a href="https://web.archive.org/web/20090308033937/http://www.voicexml.org/specs/multimodal/x+v/12/spec.html">specs trying to do something like that
          back to the early 2000's</a> and it seems to be where Google restarted the converstaion in 2012 as well.  We could figure that out, and do it with real world experimentation and testing! As you can
          see from this super rough demo, there's a lot to think about.. Should there be an attribute to
          autostart? Should it automatically stop?  Should other attributes be present? <code>paused</code>, for example? I don't know, but we could try a lot of things and see what works.
        </p>
        </section>
      


      <section class="sectioning">
        <contextual-heading role="heading" aria-level="2">So now what?</contextual-heading>
        <p>Since there's something implemented on Chrome and Android, you have a lot of
        0's in the number of machines that actually support the existing API.  As with
        a lot of 'new' features on the Web, you'll probably want think about how you can use
        progressive enhancement for a lot of use cases... In some cases, the API is
        good enough that you can paper over its kinks and that might be 'good enough'.</p>

        <p>Unfortunately, the low-level API sketched out in this document isn't possible
        to achieve <em>using the existing Speech APIs</em> because they don't provide adequate explanations
        or hooks. Fortunately, however, it is entirely plausbile to make this API real
        today using a remote service that conforms to a design like this.  Such services that transcribe
        sounds of speech to text are plentiful - Google's Cloud Platform and Amazon both
        provide APIs capable of achieving this for limited purposes would be free enough
        to experiment with and prove out, and fairly low cost beyond that.</p>

        <p>I have a kind of rough POC that I'll be building out, for purposes of demonstrating ideas to limited groups, but wide publicly sharable demonstration is potentially cost prohibitive (unless some vendor
        wants to donate free time).  However, if you have a business use for this,
        perhaps it's worthwhile.  If you're interested in helping out with that,
        or hearing more, let me know.</p>
      </section>

  

  <p class="thanksTo">Special thanks to my friend, the great Chris Wilson for proofing/commenting on pieces in this series.</p>

</article></main>

  
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-88027178-1', 'auto');
    ga('send', 'pageview');
  </script>

  <script>
    Array.prototype.slice.call(document.querySelectorAll('.optional [data-src]')).forEach(function (optional) {
      var prefs = window.localStorage.downloadsPrefs,
          clickToSee;
      if (prefs === 'true') {
        activateOptional(optional);
      } else {
        clickToSee = document.createElement('div');
        clickToSee.classList.add('clickToSee')
        clickToSee.innerHTML = '<button>Click to see media</button>';
        clickToSee.style.textAlign = 'center';
        clickToSee.style.border = 'none';
        optional.parentElement.insertBefore(clickToSee, optional.nextSibling);
        clickToSee.firstElementChild.addEventListener('click', function () {
          activateOptional(optional);
        })
      }

    })
  </script>

</body></html>