<html lang="en" resource-type="blogpost"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@briankardell">
    <meta name="twitter:creator" content="@briankardell">
    <meta name="twitter:title" content="Basic Conversation: Web Speech APIs Part IV">
    <meta name="twitter:description" content="This is part of a series about making the browser speak and listen to speech. In my last post You Don't Say: Web Speech APIs Part III, I talked about the recognition interfaces: li">
    
    <link rel="alternate" type="application/rss+xml" href="blog/feed.rss" title="bkardell.com/blog rss feed">
    <title>Basic Conversation: Web Speech APIs Part IV</title>
    <style>.captioned-image {
    background-color: #eaeaea;
    display: block;
    overflow: hidden;
    padding: 1rem;
    text-align: center;
    font-style: italic;
}

.captioned-image.p-attached {
    width: 40%;
    display: inline-block;
    margin: 0 1rem 2rem 1rem;
}

section > .captioned-image.p-attached {
    margin-top: 1.5rem; /* correct for heading size*/
}

 pre { overflow-x: auto }

.captioned-image.p-attached.p-attached-left {
    float: left;
}

.captioned-image.p-attached.p-attached-right {
    float: right;
}

.captioned-image.p-attached + * + * {
    clear: both;
    margin-top: 2rem;
}


.captioned-image img, .captioned-image video {
    display: block;
    max-width: 100%;
    margin: 0 auto 1em auto;
}
.source {
    font-style: italic;
}
body {
    display: flex;
    font: 1em "Helvetica Neue", Helvetica, Arial, sans-serif;
    font-weight: 300;
    line-height: 1.625;
}
h1 { margin: 0.5rem; }
code-format {
    border-left: 0.5rem solid rgba(123,115,209,0.35);
    display: block;
    padding: 0.5rem;
    margin-bottom: 1rem;
}

contextual-heading {
    display: block;
    margin-top: 2rem;
}

[aria-level="1"] {
    color: #43686b;
    font-size: 1.45rem;
}
[aria-level="2"] {
    color: #856363;
    font-size: 1.35rem;
}
[aria-level="3"] {
    color: #856363;
    font-size: 1.25rem;
}
[aria-level="4"] {
    color: #856363;
    font-size: 1.2rem;
}
[aria-level="5"], [aria-level="6"] {
    color: #667496;
    font-size: 1.1rem;
    font-weight: bolder;
}
.posted-on {
    font-style: italic;
    font-size: 0.8rem;
    text-align: right;
    margin-bottom: 1rem;
}

ul, li {
    margin: 0.5rem;
    text-align: left;
}
header {
    padding-top: 1rem;
    padding-right: 2rem;
    background-color: #fbf5e9;
}
.tagline {
    font-style: italic;
    text-align: center;
}

[tag-esc] {
    color: maroon;
    font-family: Courier, "Lucida Console", monospace;
}

[tag-esc]::before {
    content: '<';
}

[tag-esc]::after {
    content: '>';
}

main {
    min-width: 50%;
    width: 80%;
    padding: 1rem;
}
header .title {
    background-color: rgba(123,115,209,0.35);
    padding: 0.3rem;
    padding-left: 0.25rem;
    font-weight: bolder;
    font-size: 0.9rem;
    border-left: 1rem solid #999298;
}

header .profile {
    width: 50%;
    margin: 1rem 25%;
}

header .title + nav {
    font-size: 0.9rem;
}

header .blurb {
    font-size: 0.75rem;
    text-align: center;
}
header .name {
    text-align: center;
    font-size: xx-large;
}
.segue {
    font-style: italic;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  padding: 0;
  margin: -1px;
  overflow: hidden;
  clip: rect(0,0,0,0);
  border: 0;
}
[for="aboutMeToggle"] {
    display: none;
}
/* this needs rethinking, it wants a selector like 'not flow content' */
article>*:not(contextual-heading,a), section>*:not(contextual-heading,a) {
    margin-left: 0.45rem;
}

blockquote {
  font-family: serif;
  background: #f9f9f9;
  border-left: 10px solid #ccc;
  margin: 1.5em 10px;
  padding: 0.5em 10px;
}
blockquote:before {
  color: #ccc;
  content: open-quote;
  font-size: 2em;
  line-height: 0.1em;
  margin-right: 0.25em;
  vertical-align: -0.4em;
}

blockquote:after {
  color: #ccc;
  content: close-quote;
  font-size: 2em;
  line-height: 0.1em;
  margin-left: 0.25em;
  vertical-align: -0.4em;
}


@media (max-width: 800px){
    body {
        flex-direction: column;
    }
    header {
        border-bottom: 1px solid #afafff;
    }

    .captioned-image.p-attached {
        width: 100%;
        margin: 1rem 0;
    }

    .captioned-image.p-attached.p-attached-left,
    .captioned-image.p-attached.p-attached-right {
        float: none;
    }

    [for="aboutMeToggle"] {
        background-color: #675e5e;
        color: white;
        padding: 0.25rem;
        padding-left: 0.5rem;
        display: block;
        border-left: 1rem solid black;
    }
    #aboutMeToggle:focus + [for="aboutMeToggle"]{
        outline: 4px solid #c6ddf2;
    }

    #aboutMeToggle:checked ~ header {
        display: none;
    }

    header li { display: inline; margin-left: -0.5em; }
    header .blurb li::after { content: '; '; }
    header  nav li {
        display: inline-block;
        margin-left: 0.35rem;
        margin-right: 0.35rem;
    }
    header .title {
        width: 100%;
    }
    main li .source { display: block; }

}</style>
    <style>
      .captioned-image { font-size: 0.9rem; }
      .thanksTo { font-style: italic; font-size: 0.8rem; }
    </style>
    <script>
      var activateOptional = function (media) {
        var source = media.getAttribute('data-src'),
            temp,
            container = media.parentElement;

          if (media.nextSibling && media.nextSibling.matches) {
            if (media.nextSibling.matches('.clickToSee')) {
              media.parentElement.removeChild(media.nextSibling);
            }
          }

          if (media.tagName === 'VIDEO') {
            temp = document.createElement('video')
            temp.setAttribute('autoplay','')
            temp.setAttribute('controls','')
            temp.setAttribute('loop','')
            temp.innerHTML = '<source src="' + source + '.webm" type="video/webm">'
                    + '<source src="' + source + '.mp4" type="video/mp4">';
            media.parentElement.replaceChild(temp, media);
            setTimeout(function () {
              temp.play()
            }, 0);
          } else {
            media.src = media.getAttribute('data-src');
          }

          container.classList.add('active');
      }
    </script>
  </head>
  <body>
    <input id="aboutMeToggle" class="sr-only" type="checkbox" checked="">
<label for="aboutMeToggle">Toggle author information</label>
<header>
    <div class="name">
        Brian Kardell
    </div>
    <img class="profile" src="/profile.jpg" alt="">
    <div class="tagline"><a href="/">Betterifying the Web</a></div>
    <div class="blurb">
        <ul>
            <li>Sr Front-End Engineer at Apollo Group, Inc</li>
            <li>Original Co-author/Co-signer of The Extensible Web Manifesto</li>
            <li>Co-Founder/Chair, W3C Extensible Web CG</li>
            <li>Member, W3C (The JS Foundation)</li>
            <li>Co-author of HitchJS</li>
            <li>Blogger</li>
            <li>Art, Science &amp; History Lover</li>
            <li>Standards Geek</li>
        </ul>
    </div>
    <div class="title">Follow Me On...</div>
    <nav>
        <ul>
            <li><a rel="me" href="https://briankardell.wordpress.com">Wordpress</a></li>
            <li><a rel="me" href="https://medium.com/@briankardell">Medium</a></li>
            <li><a rel="me" href="https://twitter.com/briankardell">Twitter</a></li>
            <li><a rel="me" href="https://github.com/bkardell/">Github</a></li>
            <li><a rel="me" href="https://codepen.io/bkardell">Codepen</a></li>
            <li><a rel="me" href="https://www.linkedin.com/in/brian-kardell-08a4264">LinkedIn</a></li>
            <li><a rel="me" href="https://www.instagram.com/kardellbrian">Instagram (for
art)</a></li>
            <li><a rel="me" href="http://fineartamerica.com/profiles/brian-kardell?">Fine Art
America (art for sale)</a></li>
        </ul>
    </nav>
</header>
    <main><div class="posted-on">Posted on null</div><article class="sectioning">
  <contextual-heading role="heading" aria-level="1">Basic Conversation: Web Speech APIs Part IV</contextual-heading>
  <p class="segue">This is part of a series about making the browser speak and listen to speech. In my last post <a href="https://bkardell.com/blog/Listen-Up.html">You Don't Say: Web Speech APIs Part III</a>, I talked about the recognition interfaces: <em>listening</em> or <em>speech-to-text</em>.</p>

  <script src="../prism.js"></script>
  <link rel="stylesheet" href="../prism.css">
  <style data-id="AIzaSyBcqG9hwjurgnXOQy8yGYahRICNl2TUbuI">

    .note {
      background-color: rgba(255, 255, 0, 0.24);
      padding: 1rem;
      font-style: italic;
    }
    .note::before {
      content: 'Note';
      font-size: 0.8rem;
      background-color: black;
      color: white;
      padding: 0.25rem;
      margin-right: 0.5rem;
    }
    .output {
      margin-left: 1rem;
      font-family: "Courier New", Courier, monospace;
    }
    pre code.output {
      font-size: 0.7rem;
      border-left: 1px dotted gray;
      padding-left: 1rem;
      display: block;
    }

    .spec-quote {
      display: block;
      margin: 1rem;
    }
  </style>
  <script>
    document.body.addEventListener("click", function(evt) {
      if (evt.target.classList.contains("run")) {
        let clone = document.importNode(
          evt.target.previousElementSibling.content,
          true
        );
        document.body.appendChild(clone);
      } else if (evt.target.classList.contains("cancel")) {
        speechSynthesis.cancel();
        speechSynthesis.speak(
          new SpeechSynthesisUtterance("normalcy should be restored")
        );
      }
    });
  </script>

  <p>To sum up the state of things with recognition:</p>

  <ol>
    <li>It's <em>much</em> less widely implemented and deployed than Text-To-Speech (TTS).</li>
    <li>Like TTS, it is not an 'official' standard. Browsers are working on it, but not much work seems to be happening 'together' on this anymore.</li>
    <li>It requires special privileges</li>
    <li>Its API is very confusing</li>
  </ol>

  <p>This can seem a little disheartening, but, in fact, it's not that bad.  Since it's implemented on Chrome and Android, you have a lot of 0's in the number of your base you can experiment with and, as with a lot of features on the Web, you probably want to use progressive enhancement.  We <em>can</em> actually do a lot to experiment here and as it's not leaked out into public releases of enough other browsers, it probably means there's some room to do some rethinking.  Because of the state of this API, I think we're even more significantly in the boat <a href="https://bkardell.com/blog/Basic-Voice-Speaker.html#options">I describe in Part II</a> with regard to where we should go from here.  Far more than TTS, we don't want to add to the problem by making assumptions and the interfaces described might not be exactly ideal.</p>

  <section class="sectioning">
    <contextual-heading role="heading" aria-level="2">What's wrong with the current APIs</contextual-heading>
    <p class="note">These are just my personal opinions after using it a lot for the past couple of months.
    Sometimes eating the dogfood gives you new perspectives and nothing in this should be taken as a
    criticism of the brave souls that worked hard to bring us what we have.  It's easy to forget that we're standing on their shoulders. Hats off to them.</p>

    <p>Let's assume that the API worked consistently, as advertised by the existing draft, across browsers and devices. Would it be what we really want or need? Again, I think that, actually, the answer is no.  More so, in fact than I did with speech synthesis.  Here, I'll talk about why..
    </p>


      <section class="sectioning">
        <contextual-heading role="heading" aria-level="3">Explaining the Magic</contextual-heading>
        <p>Since the advent of <a href="https://extensiblewebmanifesto.org">The Extensible Web Manifesto</a> (EWM), we've worked very hard to help "explain the magic" already burried in features of the platform and to minimize the creation of new "magic".  Things like promises, web audio, media devices, fetch and streams are all tools in our standards bag of tricks now that we can use to commonly explain how several things work across the platform.  As the Speech API draft predatse that document and much of the subsequent work based on it, it's not really any surprise that these are not factors taken into account in the design of the APIs.  But we could fix that - there's a lot here that seems not entirely difficult to resolve.</p>

        <p class="note">In fact, the authors of the spec did recognize some of this and there are notes in the spec and errata that link to, for example <a href="https://lists.w3.org/Archives/Public/public-speech-api/2012Sep/0072.html">this observation</a> with the thought that perhaps it was too late for this draft, as it is actually a 'final report' and not an official spec.  I hope we have room to revisit this.</p>

        <p>Today, the API does a lot: It gets permission and access to 'the microphone', and then upon start 'listens' for a 'chunk of sound' followed by a pause, disengages that mic, sends that for translation, pumps related events and so on.  Much of this is now explainable by underlying features in the platform, and it should be.</p>

        <p><code>navigator.mediaDevices</code>, for example, now explains that there are lots of possible sources of audio channels carrying <code>Streams</code>, only one of which is 'the mic'.  You can
        access these with <code>navigator.mediaDevices.getUserMedia(...)</code>.  This works
        to give the developer access to streams and tracks and so on.  <em>At a minimum</em> then, we should expect this API to explain itself in terms of these sorts of now established common concepts and primitives.  The aim should ultimately not just be to document this, but <em>expose</em> the layers for extensibility.</p>

        <p>Perhaps most importantly though, the first step should be to begin by identifying and creating the truly unique features that aren't currently easily explained.  Here's why...</p>

        <section class="sectioning">
          <contextual-heading role="heading" aria-level="4">Our ability to anticipate is limited</contextual-heading>
          <p>While the API currently graciously attempts to provide a higher level, "easier" way to package up some sugar over what the group aniticipated were a few particularly common patterns - they could be wrong... And history shows that as a rule, we usually are.  We repeatedly come up short. </p>


          <p>That shouldn't surprise either.  The group of minds thinking about the problems is just too small.  In fact, it <em>can't</em> get big enough in the traditional model.  Standards in the traditional model are seemingly endless discussion and debate in terribly techincal terms.  They're words.  Lots of words.  Lots of dry, boring technical words.  More simply: For a long time, most of them don't work <em>anywhere</em> and chances are pretty good that they never will.  Frequently they die before they get a single implementation in the wild.  A <em>fast</em> standard goes from idea to widely deployed in several <em>years</em>, and it's only at this point that a lot of people really can afford to start paying attention.  When you look at it this way: Why <em>would</em> developers get involved? </p>

          <p>As long as it's limited to words, we're destined to have this problem.  Speculating with words is a very different beast than being able to touch code.  This is true no matter how hard we try.  It's not infrequent that I can convince myself that something seems good on paper: This covers all the use cases eleoquently.  But when I code it up and then spend an increasingly long timewith it, I eventually inevitably find out that I've fallen short and need to iterate.  And that's just me and a really limited amount of time.  How much more does this affect the model of stanards on paper?  Having 0 implementations or even just one behind a flag that can't be emulated just really limits the imagination, and the number of people who can afford to imagine.</p>


          <p>However, if we instead focusing on giving very basic powers to developers, we are doing two things:  First, it's a smaller ask.  It's targeted.  We can get it done and widely deployed faster.  Second, this allows a significantly wider community brainstorm in <em>practice</em>.  They can to string together not just one but several ideas far beyond what a few people in a group might otherwise imagine. They can find the shortcomings. They can find new ways around the shortcomings.</p>
        </section>

        <section class="sectioning">
          <contextual-heading role="heading" aria-level="4">Missed value</contextual-heading>
          <p>So, this is all well and good in theory, but what about in practice?  Sure, I can say the API is wonky, but "wonky" is a lot different than "can't meet use cases" or "misses important value".  I can say "it doesn't expose its patterns in an extensible way" but as I explain in <a href="http://bkardell.com/blog/Tao-Of-The-Extensible-Web.html">What Would Bruce Lee Do? The Tao of the Extensible Web</a>, in some cases we can back into that - we can come back and expose details of the black box if we're careful about it.  So, the real question is: If we had these APIs today, <em>would I find myself unable to do things I'd like to do because of this?</em> The answer, I think, is yes.</p>

          <p>When I hear "voice recognition" or "speech to text", this implies to me that I can take a sound,
          analyze it and turn it into a transcipt of text.  I watch a demo and I see someone speak into the mic and words come out.  I get excited and begin thinking of the possibilities. Yes, I want that.  Think of all the things I could do!
          </p>

          <p>I have a thing that downloads podcasts and talks that I might want to listen to in the future.  These have... wait for it... <em>sounds of voices</em>.  If I could turn those into text, I could do lots of interesting things - skim them for example to see if I want to listen.  Better still, I can index them and hyperlink so that I can search for things that were discussed without trying to remember what was said about Web Components in ..some a podcast? with Jeremy Keith I think? Last year maybe? In fact, I don't even have to watch it to know that.  Doesn't google do that? Sure, to some extent, but that doesn't allow me optimize for things like the collection of stuff I've already got.  You can't do that with this API.</p>

          <p>Speaking of podcasts, I've been thinking of doing one.  Imagine I'm in a studio with 4 other people all with their own mics.  It'd be super neat to identify them by name and capture their transcribed text in a simple transcript.  Hey, that's kind of like meetings.  I attend a lot of meetings that aren't minuted.  We could fix that.  But, well, not with this API.</p>

          <p>I also have a thing that lets me upload a video to a website I maintain... I know that for accessibility sake I need captions of this video and that has to be in Timed Text Markup Language (TTML).  Wow, that's a pain in the ass to write from scratch.  I could write a thing to take the audio track, turns chunks of it into text, inserts that into TTML and uploads me a draft in my workflow that I can then simply proof, correct,  approve and publish.  We can't do that with this API either.</p>

          <p>I also had an idea for simplistic, hands-free conversational interface for while I am driving.  The current coupling with the microphone without a convenient API actually made this very difficult as it was very easy to have it hear itself responding and get stuck in a loop.  What I'd really like is a simple circuit breaker kind of ability on the lines themselves.  While I did seem to work around this eventually, the current design made this very hard.</p>

          <p>These are just a few of the things I very quickly came up with but was disappointed I could not accomplish.  Given actual powers you'd probably be astonished at what we can imagine.  I've <a href="http://bkardell.com/blog/Prognostication-And-The-Failure-Of-The-Web.html">written about this before</a>, but it's astonishing how many creations that are fundamental things today turned out to <em>not</em> be what their inventors even thought they would be. Houses were originally wired for artifical lighting  - there were no plugs, because there was nothing to plug in.  While we were busy standardizing bulbs, an astonishing flourishing of inventions and whole new industries were created by the simple fact that we gave people electricity.</p>

          <p>So, let's find the missing powers and give those to developers.  As far as I can tell, really the only thing in these APIs that is fundamentally new is the idea of an interface for transcibing audio of speech into text.  If you think about it, we've effectively got all the parts of everything else today.  Most of the events and errors in the proposed interface deal with things like grabbing the mic, starting to listen and so on. We've got those already for the most part.</p>

          <p>A few, like <code>.onsoundstart</code> and <code>.onsoundend</code> seem like they are about responding to chunking up as actual sounds that pass some criteria come in.  I'm not sure we have this, but I'm pretty sure we could build it with existing APIs and that if we did, it too could be generally useful in all sorts of contexts, not just this one.  Similarly, <code>.onspeechstart</code> or <code>.onspeechend</code> are just a special sort of criteria of the above where the criteria is based on
        trying to detect <em>speech</em> in the sounds. I don't think we have anything like this formalized
        today, but again, I think we do have the parts we need to build it and explain.</p>

        <p>No, I think <em>the big, missing bit is simply a speech transcription API</em>...</p>
      </section>

      <section class="sectioning">
        <contextual-heading role="heading" aria-level="4">A transcription interface</contextual-heading>
        <p>At the end of the day, transcription of words seems like what we want, so let's imagine that we started with that idea: An API that given some audio, gives you back the transcribed text.  Accessing your filesystem or your mic are things that requires permission, but what we're talking about here doesn't itself do that, so we don't have to deal with any of that.  We can follow the common model of being simple async and transactional, pass/fail things that we deal with via promises - perhaps something like:</p>

        <pre><code class="language-javascript">
  // let's save bikeshedding a name for another day...
  let transcriber = new FictionalTranscriber()

  transcriber.transcribe(someAudioStream).then((response) =&gt; {
    // we'll talk more about this
  })
        </code></pre>

        <p>That's pretty straightfoward I think. A transcriber has a <code>response</code> and an error is handled with the promise's <code>.catch</code> method.</p>

        <section class="sectioning">
          <contextual-heading role="heading" aria-level="5">Results</contextual-heading>
          <p>In the current interface you get these wonky array-likes which default to having exactly one
          item in them.  Let's kind of work backwards... Imagine that your service response itself contains
          the properties <code>.transcript</code> and <code>.confidence</code> (from the existing APIs).  Then the default 'just give me your best guess' case is as simple as:</p>

          <pre><code class="language-javascript">
  let service = new FictionalTranscriber()

  transcriber.transcribe(someAudioStream).then((response) =&gt; {
    console.log(`
      I am ${response.confidence}% confident
      that I heard "${response.transcript}".
    `)
  })
        </code></pre>

          <p>The existing APIs contain a separate event called <code>onnomatch</code> that is
          different from <code>onerror</code> in that it's more about it receiving
          something and processing it but not being able to confidently enough
          deterimine what was said. It seems that what we mean to say is "we recognized nothing" or
          "we heard something but what it was is undefined".  In both cases these have
          programming concepts for these: The null string ("") or, in JavaScript we also have <code>undefined</code>.  In JavaScript are falsey - so I think we can get rid of that extra event
          entirely with something as simple as saying that that is a possiblility...</p>


        <pre><code class="language-javascript">
  let transcriber = new FictionalTranscriber()

  transcriber.transcribe(someAudioStream).then((response) =&gt; {
    // if we didn't match anything sufficiently
    if (!response.transcript) {
       console.log(`Sorry, I didn't catch that`)
    } else {
      console.log(`
        I am ${response.confidence}% confident
        that I heard "${response.transcript}".
      `)
    }

  })
        </code></pre>


          <section class="sectioning">
            <contextual-heading role="heading" aria-level="6">Alternatives</contextual-heading>
            <p>This model could also allow our response to contain an <code>.alternatives</code> array
          beyond our best guess transcript.  This seems to actually
          match the meaning of the word "alternative" better anyway IMO.
          By default, it would be empty because there are no alternatives. Asking, for them,
          again, seems like simply part of the service.</p>

          <pre><code class="language-javascript">
let service = new FictionalTranscriptionService({maxAlternatives: 10})

service.transcribe(someAudioStream).then((response) =&gt; {
  console.log(`
    I am ${response.confidence}% confident
    that I heard "${response.transcript}".
  `)

  if (response.alternatives) {
    console.log(`But here are ${evt.alternatives} alternatives... `)
    response.alternatives.forEach((alternative) =&gt; {
      console.log(`
        I am ${alternative.confidence}% sure it could
        have been "${alternative.transcript}"
      `)
    })
  }

})
          </code></pre>
          </section>

          <section class="sectioning">
            <contextual-heading role="heading" aria-level="6">Implementation Flexibility and Black Boxing</contextual-heading>
            <p>One of the extra handy things about this kind of design, I think, is that it also neatly sidesteps a number of problems that are impossibly difficult to resolve at this stage by boxing those problems at an appropriate place where there is easier to reach value and yet room for experimentation.  Let me explain...</p>

            <p>What we are providing what is, effectively, a service interface.  We're not saying how the actual
            transciption should be done.  There are an astounding amount of challenges there.  There are lots of different existing remote services with lots of different qualities, lots of different APIs and lot of different sorts of inputs. A want to figure all of this out will stall a process for a very long time and be, ultimately, as much about politics and sway as actual merit.</p>

            <p>On the other hand, if we simply say "Here's what it has to look like on the client, and the client should include a default implementation" we have to agree to very little.  While this seems like it isn't giving you 'much', the adoption of this simple pattern would provide the standard we most desparately need:  All we have to agree on at this stage is what the client interface looks like.</p>

            <p>There are some other practical upshots to this too.  The first is that we can also make that extensible by defining basic API lifecycle methods and allow competition and variance to play itself out very easily and employ lines of thinking we can't easily consider in a standards body.  Imagine, for example, that Google's cloud platform could release a subclass:</p>

            <pre><code class="language-javascript">
export default class FictionalGoogleCloudTranscriptionService
  extends FictionalTranscriptionService {
    // override
    request: (audio) =&gt; {
      // return some promise that meets the expected API
      return fetch(....).then(...)
    }
}
          </code></pre>

          <p>The only thing that changes is which one you instantiate - they all work, at the API level, the same way.  This means that these adapters are then as easily distributable as jQuery plugins and Amazon can compete with Google and Microsoft and Mozilla and... well... anyone while we figure out </p>

          <p>This also fits very nicely with allowing us to experiment with the remaining (seemingly unimplemented) ideas in the APIs (like <em>grammars</em>) and provides room for us to work things out.  Eventually, perhaps we can derive a higher level protocol allowing us to express things as simply as:</p>

          <pre><code class="language-javascript">
let service = new FictionalTranscriptionService({
  serviceURI: 'blah',
  grammars: whateverThisIs
})

service.transcribe(someAudioStream).then((response) =&gt; {
  console.log(`
    I am ${response.confidence}% confident
    that I heard "${response.transcript}".
  `)
})
    </code></pre>

            <p>Importantly though, this means we don't have to wait for that day and we can be part of the solution in figuring out what it should be.</p>
          </section>
        </section>
        </section>
      </section>


      </section>



      <section class="sectioning">
        <contextual-heading role="heading" aria-level="2">So now what?</contextual-heading>

        <p>I think this would be pretty good, but unfortunately, these are all
        pretty low level asks that are currently pretty difficult to fill in with the native
        implementations themselves because they don't provide adequate explanations
        or hooks.  For my own use cases, the native APIs are
        'adequate' to paper over with a simpler interface that simply avoids all
        of the array and event weirdness that can occur, so that's
        what I am currently choosing to do in most of my use cases.</p>

        <p>It is plausbile, however, to make this API real today using a
        remote service that conforms to this.  Such services that transcribe
        sounds of speech to text are plentiful - Google's Cloud Platform and Amazon both
        provide APIs capable of achieving this for limited purposes would be free enough
        to experiment with and prove out, and fairly low cost beyond that.</p>

        <p>I have a kind of rough POC that I'll be building out,
        for purposes of demonstrating ideas to limited groups, but wide publicly
        sharable demonstration is potentially cost prohibitive (unless some vendor
        wants to donate free time).  However, if you have a business use for this,
        perhaps it's worthwhile.  If you're interested in helping out with that,
        or hearing more, let me know.</p>
      </section>

  

  <p class="thanksTo">Special thanks to my friend, the great Chris Wilson for proofing/commenting on pieces in this series.</p>

  <!-- script src="https://rawgit.com/mattdiamond/Recorderjs/master/dist/recorder.js"></script -->

</article></main>

  
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-88027178-1', 'auto');
    ga('send', 'pageview');
  </script>

  <script>
    Array.prototype.slice.call(document.querySelectorAll('.optional [data-src]')).forEach(function (optional) {
      var prefs = window.localStorage.downloadsPrefs,
          clickToSee;
      if (prefs === 'true') {
        activateOptional(optional);
      } else {
        clickToSee = document.createElement('div');
        clickToSee.classList.add('clickToSee')
        clickToSee.innerHTML = '<button>Click to see media</button>';
        clickToSee.style.textAlign = 'center';
        clickToSee.style.border = 'none';
        optional.parentElement.insertBefore(clickToSee, optional.nextSibling);
        clickToSee.firstElementChild.addEventListener('click', function () {
          activateOptional(optional);
        })
      }

    })
  </script>

</body></html>